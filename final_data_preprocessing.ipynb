{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import json\n",
    "import itertools\n",
    "import datetime \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path variables for KEVIN\n",
    "\n",
    "path = \"D:\\Kevin Liang/Documents/1_UT_SENIOR/UT_AUSTIN_FALL_2017/EE_379K/Data_Science_Final_Project/Kickstarter_Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_results(query):\n",
    "    browser = webdriver.Firefox()\n",
    "    browser.get(query)\n",
    "    time.sleep(1)\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "    while True:\n",
    "        elemsCount = browser.execute_script(\"return document.querySelectorAll('.stream-items > li.stream-item').length\")\n",
    "\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(browser, 20).until(\n",
    "                lambda x: x.find_element_by_xpath(\n",
    "                    \"//*[contains(@class,'stream-items')]/li[contains(@class,'stream-item')][\"+str(elemsCount+1)+\"]\"))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    errors = browser.find_elements_by_class_name('center')\n",
    "    for error in errors:\n",
    "        if (error.text == \"Sorry! This person is no longer active on Kickstarter.\"):\n",
    "            browser.quit()\n",
    "            return 0, 0\n",
    "\n",
    "    price = browser.find_elements_by_class_name('count')\n",
    "    check = False\n",
    "    for val in price:\n",
    "        count = val.text\n",
    "        count =count.encode('utf-8')\n",
    "        if (check == True):\n",
    "            num_backed= count\n",
    "            check = False\n",
    "        else:\n",
    "            num_created = count\n",
    "            check = True\n",
    "            \n",
    "    '''  \n",
    "    projects = browser.find_elements_by_class_name('backed')\n",
    "    for project in projects:\n",
    "        text = project.text\n",
    "        words = text.split()\n",
    "        num_backed = str(words[1])\n",
    "        #print num_backed\n",
    "        browser.quit()\n",
    "        \n",
    "    '''  \n",
    "    \n",
    "    return int(num_backed), int(num_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_user_profile_data(df):\n",
    "    user_num_backers = []\n",
    "    user_num_creators = []\n",
    "    \n",
    "    for user_url in df['user_profiles']:\n",
    "        b, c = search_results(user_url)\n",
    "        user_num_backers.append(b)\n",
    "        user_num_creators.append(c)\n",
    "    \n",
    "    df['user_num_backers'] = pd.Series(user_num_backers)\n",
    "    df['user_num_creators'] = pd.Series(user_num_creators)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data - Data set\n",
    "# feature - feature that is going to be processed using nlp\n",
    "\n",
    "def nlp_data(data, feature, total_words):\n",
    "    all_words = []\n",
    "    regex_word = \"[^a-zA-Z]\"\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for cell_data in data[feature]:\n",
    "        words = re.sub(regex_word,\" \",cell_data).lower().split()\n",
    "        words = [i for i in words if not i in stopwords.words('english')]\n",
    "        # might change len of word\n",
    "        words = [i for i in words if len(i) > 1]\n",
    "        words = [lem.lemmatize(i, \"v\") for i in words]\n",
    "        words = [lem.lemmatize(i) for i in words]\n",
    "        all_words += words\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", stop_words = \"english\", max_features = total_words)\n",
    "    word_count = vectorizer.fit_transform(all_words).toarray()\n",
    "    final_words = vectorizer.get_feature_names()\n",
    "    \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_nlp(data, feature):\n",
    "    word_list = []\n",
    "    regex_word = \"[^a-zA-Z]\"\n",
    "    lem = WordNetLemmatizer()\n",
    "    for i in data[feature]:\n",
    "        words = re.sub(regex_word,\" \",i).lower().split()\n",
    "        words = [i for i in words if not i in stopwords.words('english')]\n",
    "        # might change len of word\n",
    "        words = [i for i in words if len(i) > 1]\n",
    "        words = [lem.lemmatize(i, \"v\") for i in words]\n",
    "        words = [lem.lemmatize(i) for i in words]\n",
    "        word_list.append(words)\n",
    "    data.drop(feature, axis = 1)\n",
    "    data[feature] = word_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp(data, list_of_features, total_words):\n",
    "    for i in list_of_features:\n",
    "        final_words = nlp_data(data, i, total_words)\n",
    "        data = preprocess_nlp(data,i)\n",
    "        nlp_features = []\n",
    "        for word in final_words:\n",
    "            lst = []\n",
    "            for x in data[i]:\n",
    "                lst.append(x.count(word))\n",
    "            nlp_features.append(lst)\n",
    "            \n",
    "        nlp_features = np.array(nlp_features)\n",
    "        nlp_features_column = np.array(nlp_features).T\n",
    "        nlp_feature_data = pd.DataFrame(data = nlp_features_column, columns = final_words)\n",
    "        data = data.drop(i, axis = 1)\n",
    "        data = pd.concat([data, nlp_feature_data], axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data - dataset as a DataFrame object\n",
    "# list_of_features - features that need to be one hot encoded\n",
    "\n",
    "def ohe_data(data, list_of_features):\n",
    "    ohe = data[list_of_features]\n",
    "    new_data = data.drop(list_of_features, axis = 1)\n",
    "    ohe_features = pd.get_dummies(ohe)\n",
    "    return pd.concat([new_data,ohe_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_creator_parser(data):\n",
    "    new_feature = []\n",
    "    for i in range(data[\"creator\"].shape[0]):\n",
    "        json_parsed_data = json.loads(data[\"creator\"].iloc[i])\n",
    "        new_feature.append(json_parsed_data['urls']['web']['user'])\n",
    "        \n",
    "    new_data = data.drop('creator', axis = 1)\n",
    "    new_data[\"user_profiles\"] = new_feature\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_category_parser(data):\n",
    "    new_feature_category = []\n",
    "    new_feature_subcategory = []\n",
    "    for i in range(data[\"category\"].shape[0]):\n",
    "        json_parsed_data = json.loads(data['category'].iloc[i])\n",
    "        new_feature_category.append(json_parsed_data['slug'].split('/')[0])\n",
    "        new_feature_subcategory.append(json_parsed_data['slug'].split('/')[1])\n",
    "        \n",
    "    new_data = data.drop('category', axis = 1)\n",
    "    new_data[\"category\"] = new_feature_category\n",
    "    new_data[\"subcategory\"] = new_feature_subcategory\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + \"combined_data.csv\", header = 0, index_col = 0)\n",
    "data = data.drop([\"photo\", \"urls\",\"source_url\",\"location\"], axis = 1)\n",
    "total_time = abs(data[\"launched_at\"] - data[\"deadline\"])\n",
    "data[\"total_time\"] = total_time\n",
    "nlp_features = [\"blurb\",\"name\"]\n",
    "ohe_features = [\"state\",\"disable_communication\",\"country\",\"currency\", \"currency_trailing_code\",\"is_starrable\",\"current_currency\",\"usd_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157127, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nlp(data,nlp_features, 100)\n",
    "data = ohe_data(data, ohe_features)\n",
    "data = json_category_parser(data)\n",
    "data = json_creator_parser(data)\n",
    "#data = add_user_profile_data(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv(path + \"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   0\n",
       "id                           0\n",
       "name                         1\n",
       "blurb                       10\n",
       "goal                         0\n",
       "pledged                      0\n",
       "state                        0\n",
       "disable_communication        0\n",
       "country                      0\n",
       "currency                     0\n",
       "currency_trailing_code       0\n",
       "deadline                     0\n",
       "created_at                   0\n",
       "launched_at                  0\n",
       "is_starrable                 0\n",
       "usd_pledged                  0\n",
       "converted_pledged_amount     0\n",
       "current_currency             0\n",
       "usd_type                     0\n",
       "creator                      0\n",
       "category                     0\n",
       "total_time                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'successful': 73688, 'failed': 70556, 'canceled': 8680, 'live': 3591, 'suspended': 612})\n"
     ]
    }
   ],
   "source": [
    "state = Counter(data[\"state\"])\n",
    "print state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157127, 22)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
